{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f124aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Image\n",
    "import tensorflow as tf\n",
    "from skimage import transform\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e607dfb",
   "metadata": {},
   "source": [
    "# Data import & Pre-processing --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6396e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets paths for images \n",
    "def get_catalog(file):\n",
    "    species = []\n",
    "    pics = []\n",
    "    for root, subdir, files in os.walk(file):\n",
    "        species.append(subdir)\n",
    "        pics.append(files)\n",
    "    species = species[0]\n",
    "    \n",
    "    catalog = {}\n",
    "    for i in range(len(species)):\n",
    "        catalog[species[i]] = pics[i+1]\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8292e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(picture, save_loc,name):\n",
    "    print(picture.shape)\n",
    "    plt.imshow(picture)\n",
    "    plt.show()\n",
    "    flip_2 = tf.image.flip_up_down(picture)\n",
    "    print(flip_2.shape)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'a.jpg', np.array(flip_2))\n",
    "    flip_3 = tf.image.flip_left_right(picture)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'b.jpg', np.array(flip_3))\n",
    "    flip_4 = tf.image.flip_up_down(flip_3)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'c.jpg', np.array(flip_4))\n",
    "    rot_90 = tf.image.rot90(picture, k=1)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'd.jpg', np.array(rot_90))\n",
    "    rot_180 = tf.image.rot90(picture, k=3)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'e.jpg', np.array(rot_180))\n",
    "    rot_2 = tf.image.rot90(flip_3, k=1)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'f.jpg', np.array(rot_2))\n",
    "    rot_3 = tf.image.rot90(flip_3, k=3)\n",
    "    cv2.imwrite(save_loc + name[:-4] + 'g.jpg', np.array(rot_3))\n",
    "   #rot = transform.rotate(picture, angle=45, mode='reflect')\n",
    "   #plt.imshow(rot)\n",
    "   #plt.show()\n",
    "   #rot1 = transform.rotate(rot, angle=90, mode='reflect')\n",
    "   #plt.imshow(rot1)\n",
    "   #plt.show()\n",
    "   #rot2 = transform.rotate(rot1, angle=90, mode='reflect')\n",
    "   #plt.imshow(rot2)\n",
    "   #plt.show()\n",
    "   #rot3 = transform.rotate(rot2, angle=90, mode='reflect')\n",
    "   #plt.imshow(rot3)\n",
    "   #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd330fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pics(catalog,path, species_index,pic_index, size):\n",
    "    count = 1\n",
    "    new_catalog = {}\n",
    "    for specie in catalog:\n",
    "        images =[]\n",
    "        for i in range(pic_index):\n",
    "            image = cv2.imread(path + specie + '/' + catalog[specie][i])\n",
    "            data_augmentation(image,path + specie + '/',catalog[specie][i])\n",
    "            #res = cv2.resize(image, dsize=size, interpolation=cv2.INTER_NEAREST)\n",
    "            #images.append(res.flatten())\n",
    "        #new_catalog[specie] = np.array(images)\n",
    "        if count == species_index:\n",
    "            break\n",
    "        count += 1\n",
    "        \n",
    "    return new_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_pics2(path, nspec, npic,shape):\n",
    "    bird_catalog = get_catalog(path)\n",
    "    pics = get_pics(bird_catalog,path, nspec, npic,shape)\n",
    "    return pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92383a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(pictures):\n",
    "    test_images = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for species, values in pictures.items():\n",
    "        if len(test_images) == 0:\n",
    "            test_images = values\n",
    "            labels = np.repeat(count,values.shape[0])\n",
    "        else:\n",
    "            test_images = np.concatenate((test_images,values),axis=0)\n",
    "            labels = np.concatenate((labels, np.repeat(count, values.shape[0])),axis=0)\n",
    "        count +=1\n",
    "    test_images = np.array(test_images)\n",
    "    return test_images, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(random_state=seed).fit(X)\n",
    "    new_X = pca.transform(X)\n",
    "    print(\"Dim reduction ready.\")\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51829477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction_kernelcosine(X):\n",
    "    kpca = KernelPCA(kernel='cosine',random_state=seed).fit(X)\n",
    "    new_X = kpca.transform(X)\n",
    "    print(\"Kernel PCA cosine ready\")\n",
    "    return new_X, kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction_kernelpoly(X):\n",
    "    kpca = KernelPCA(kernel='poly',random_state=seed).fit(X)\n",
    "    new_X = kpca.transform(X)\n",
    "    print(\"Kernel PCA poly ready\")\n",
    "    return new_X, kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67476861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for data preprocessing ----------------------------------------------------------------------------------\n",
    "seed = 2\n",
    "np.random.RandomState(seed)\n",
    "\n",
    "#Declare variables \n",
    "train_file, valid_file, test_file = 'data/birds/train/', 'data/birds/valid/', 'data/birds/test/'\n",
    "num_species = 20\n",
    "num_pics_train, num_pics_val, num_pics_test = 120, 5, 5\n",
    "size = (64,64)\n",
    "\n",
    "#get pictures\n",
    "train_pictures = import_pics2(train_file, num_species, num_pics_train,size)\n",
    "#valid_pictures = import_pics2(valid_file, num_species, num_pics_val,size)\n",
    "#test_pictures = import_pics2(test_file, num_species, num_pics_test,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2803c06",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigendecomposition(images):\n",
    "    species_values = {}\n",
    "    for specie, values in images.items():\n",
    "        mean = np.mean(values,axis=0)\n",
    "        x = (values-mean)/ 255\n",
    "        U, S, V = np.linalg.svd(x.T,full_matrices=False)\n",
    "        species_values[specie] = [U, S, mean]\n",
    "    print(\"Done eigendecomposing.\")\n",
    "    return species_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenspecies(eigen,n_comp):\n",
    "    species_eigenvectors = {}\n",
    "    for spec, vals in  eigen.items():\n",
    "        U = vals[0]\n",
    "        mean = vals[2]\n",
    "        eigenvs =  U[:,:n_comp]\n",
    "        species_eigenvectors[spec] = (eigenvs, mean)\n",
    "    print(\"Done getting eigenspecies.\")\n",
    "    return species_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(eigenspecies,test):\n",
    "    residuals_matrix = []\n",
    "    test = test / 255\n",
    "    matrix_residual = []\n",
    "    for image in test:\n",
    "        per_species_residual = []\n",
    "        for specie, metrics in eigenspecies.items():\n",
    "            pre_image = image - (metrics[1]/255)\n",
    "            residual = np.linalg.norm(pre_image - (metrics[0] @ metrics[0].T @ pre_image))**2\n",
    "            per_species_residual.append(residual)\n",
    "        matrix_residual.append(per_species_residual) \n",
    "    print(\"Done getting residuals.\")\n",
    "    return np.array(matrix_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "eigenvectors = eigendecomposition(train_pictures)\n",
    "eigenspecies = get_eigenspecies(eigenvectors, 90)\n",
    "X_test, y_test = get_Xy(test_pictures)\n",
    "residuals = get_residuals(eigenspecies,X_test)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = residuals.argmin(axis=1) + 1\n",
    "print(accuracy_score(y_test,classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ec464",
   "metadata": {},
   "source": [
    "# Density Estimation -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(cluster_labels,y_train):\n",
    "    reference_labels = {}\n",
    "    \n",
    "    for i in range(len(np.unique(cluster_labels))):\n",
    "        index = np.where(cluster_labels == i,1,0)\n",
    "        num = np.bincount(y_train[index==1]).argmax()\n",
    "        reference_labels[i] = num\n",
    "    return reference_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(seed,num_clust,X,y):\n",
    "    clf_km = KMeans(n_clusters=num_clust,random_state=seed).fit(X)\n",
    "    map_kmlabels = retrieve_info(clf_km.labels_,y)\n",
    "    km_labels = [map_kmlabels[i] for i in clf_km.labels_]\n",
    "    return map_kmlabels, clf_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_k_means(seed,num_clust,X,y):\n",
    "    clf_skm = SphericalKmeans(n_clusters=num_clust)\n",
    "    map_kmlabels = retrieve_info(clf_skm.labels_,y)\n",
    "    km_labels = [map_kmlabels[i] for i in clf_km.labels_]\n",
    "    return map_kmlabels, clf_km\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_Xy(train_pictures)\n",
    "X_test, y_test = get_Xy(test_pictures)\n",
    "\n",
    "#X_pctrain, dim_reductor = dim_reduction(X_train)\n",
    "#X_pctest = dim_reductor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping, model = k_means(seed, 125, X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "translated_pred = [mapping[i] for i in pred]\n",
    "print(accuracy_score(translated_pred, y_test))\n",
    "\n",
    "#mapping2, model2 = sphere_k_means(seed, 125, X_train, y_train)\n",
    "#pred2 = model2.predict(X_test)\n",
    "#translated_pred2 = [mapping2[i] for i in pred2]\n",
    "#print(accuracy_score(translated_pred2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "cluster = []\n",
    "sil = []\n",
    "for i in np.arange(2,150,5):\n",
    "    mapping, model = k_means(seed, i, X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    translated_pred = [mapping[i] for i in pred]\n",
    "    sil.append(silhouette_score(X_train, model.labels_, metric = 'euclidean', random_state=seed))\n",
    "    accuracy.append(accuracy_score(translated_pred, y_test))\n",
    "    cluster.append(i)\n",
    "    print('Iteration ' + str(i) + \" complete.\")\n",
    "plt.plot(cluster,sil)\n",
    "plt.show()\n",
    "plt.plot(cluster, accuracy)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340585e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "translated_pred = [mapping[i] for i in pred]\n",
    "print(accuracy_score(translated_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce8fca",
   "metadata": {},
   "source": [
    "# Traditional Classification --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, label):\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    n_neighbors = {'n_neighbors': list(range(1, 31))}\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=1000,random_state=seed)\n",
    "    \n",
    "    svm = SVC()\n",
    "    kernel = {'kernel':('linear','rbf'),'C': [0.1, 1, 10, 100],'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "    \n",
    "    clf_svm = GridSearchCV(svm,kernel).fit(X,label)\n",
    "    print(\"SVM done.\")\n",
    "    clf_knn = GridSearchCV(knn,n_neighbors).fit(X,label)\n",
    "    print(\"KNN done\")\n",
    "    \n",
    "    print(clf_knn.best_score_)\n",
    "    print(clf_svm.best_score_)\n",
    "    print(np.mean(cross_val_score(lr,X,label,cv=5)))\n",
    "    print(\"LR done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c98980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, label):\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = get_Xy(train_pictures)\n",
    "X, kpca = dim_reduction_kernelcosine(X)\n",
    "train_model(X, y)\n",
    "\n",
    "X, y = get_Xy(train_pictures)\n",
    "X, kpca = dim_reduction_kernelpoly(X)\n",
    "train_model(X, y)\n",
    "\n",
    "\n",
    "X, y = get_Xy(train_pictures)\n",
    "X, pca = dim_reduction(X)\n",
    "train_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d2bf9",
   "metadata": {},
   "source": [
    "# Deep Learning -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb12c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_pre_process(image_catalog, og_shape):\n",
    "    X, y = get_Xy(image_catalog)\n",
    "    tenso_X = np.reshape(X,(X.shape[0], og_shape[0], og_shape[1], 3))\n",
    "    tenso_X = tenso_X.astype('float32')\n",
    "    tenso_X = tenso_X / 255.\n",
    "    \n",
    "    y_one_hot = to_categorical(y)\n",
    "    return tenso_X, y_one_hot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = deep_pre_process(train_pictures, size)\n",
    "X_val, y_val = deep_pre_process(valid_pictures, size)\n",
    "X_test, y_test = deep_pre_process(test_pictures, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f97206",
   "metadata": {},
   "source": [
    "## Neural Network implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62337df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape,classes):\n",
    "    bird_model = Sequential()\n",
    "    bird_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(shape[1],shape[2],shape[3]),padding='same'))\n",
    "    bird_model.add(LeakyReLU(alpha=0.1))\n",
    "    bird_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "    bird_model.add(Dropout(0.25))\n",
    "    bird_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "    bird_model.add(LeakyReLU(alpha=0.1))\n",
    "    bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    bird_model.add(Dropout(0.25))\n",
    "    bird_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "    bird_model.add(LeakyReLU(alpha=0.1))                  \n",
    "    bird_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    bird_model.add(Dropout(0.4))\n",
    "    bird_model.add(Flatten())\n",
    "    bird_model.add(Dense(128, activation='linear'))\n",
    "    bird_model.add(LeakyReLU(alpha=0.1))           \n",
    "    bird_model.add(Dropout(0.3))\n",
    "    bird_model.add(Dense(classes, activation='softmax'))\n",
    "    \n",
    "    bird_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(),metrics=['accuracy'])\n",
    "    print(bird_model.summary())\n",
    "    return bird_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8019dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = num_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d510f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cnn = build_model(X_train.shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_train = clf_cnn.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = clf_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70972ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = bird_train.history['accuracy']\n",
    "val_accuracy = bird_train.history['val_accuracy']\n",
    "loss = bird_train.history['loss']\n",
    "val_loss = bird_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0935efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
